{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import faiss"
      ],
      "metadata": {
        "id": "a_ecrCvg_FpT"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CHUNK_SIZE = 300          # размер чанка в словах\n",
        "CHUNK_OVERLAP = 50        # перекрытие между чанками\n",
        "MAX_FEATURES = 20000      # размер словаря TF-IDF\n",
        "MAX_DF = 0.95             # игнорировать очень частые термы\n",
        "NGRAM_RANGE = (1, 2)      # униграммы + биграммы\n",
        "TOP_K_CHUNKS = 20         # сколько кандидатов чанков искать на вопрос\n",
        "TOP_N_WEBIDS = 5          # сколько web_id отдавать в submit\n",
        "\n",
        "WEBSITES_PATH = \"websites.csv\"\n",
        "QUESTIONS_PATH = \"questions_clean.csv\"\n",
        "SUBMIT_PATH = \"submit.csv\""
      ],
      "metadata": {
        "id": "RvsdoeZk_IZs"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Очистка текста\"\"\"\n",
        "    text = str(text)\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip().lower()\n",
        "\n",
        "def chunk_text(text: str, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):\n",
        "    \"\"\"Разбиваем текст на чанки с перекрытием\"\"\"\n",
        "    words = str(text).split()\n",
        "    if not words:\n",
        "        return []\n",
        "    chunks = []\n",
        "    step = max(chunk_size - overlap, 1)\n",
        "    for i in range(0, len(words), step):\n",
        "        chunk = \" \".join(words[i:i+chunk_size])\n",
        "        if chunk.strip():\n",
        "            chunks.append(chunk)\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "pZp8YqZU_Nf4"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузка данных"
      ],
      "metadata": {
        "id": "V9pU7JAhBpm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "websites_df = pd.read_csv(WEBSITES_PATH)\n",
        "questions_df = pd.read_csv(QUESTIONS_PATH)\n",
        "\n",
        "required_web_cols = {\"web_id\", \"text\"}\n",
        "required_q_cols = {\"q_id\", \"query\"}\n",
        "assert required_web_cols.issubset(set(websites_df.columns)), f\"В websites.csv нет колонок: {required_web_cols - set(websites_df.columns)}\"\n",
        "assert required_q_cols.issubset(set(questions_df.columns)), f\"В questions_clean.csv нет колонок: {required_q_cols - set(questions_df.columns)}\"\n",
        "\n",
        "print(f\"Страниц: {len(websites_df)}, Вопросов: {len(questions_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Hp0DbKe_QiQ",
        "outputId": "a5591812-1a18-432f-8443-0eb0f552ee0b"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Страниц: 1937, Вопросов: 6977\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чанкование"
      ],
      "metadata": {
        "id": "d3nO5eJD_Upu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_rows = []\n",
        "for _, row in websites_df.iterrows():\n",
        "    web_id = row[\"web_id\"]\n",
        "    raw_text = row[\"text\"]\n",
        "    cleaned = clean_text(raw_text)\n",
        "    chunks = chunk_text(cleaned)\n",
        "    for ch in chunks:\n",
        "        chunk_rows.append({\"web_id\": web_id, \"chunk_text\": ch})\n",
        "\n",
        "chunks_df = pd.DataFrame(chunk_rows)\n",
        "print(f\"Получено чанков: {len(chunks_df)} (из {len(websites_df)} страниц)\")\n",
        "\n",
        "# фильтруем пустые чанки и слишком короткие\n",
        "if len(chunks_df) == 0:\n",
        "    raise ValueError(\"После чанкования нет ни одного чанка. Проверь содержимое websites.csv['text'].\")\n",
        "\n",
        "chunks_df[\"chunk_text_len\"] = chunks_df[\"chunk_text\"].str.len()\n",
        "chunks_df = chunks_df[chunks_df[\"chunk_text_len\"] > 20].drop(columns=[\"chunk_text_len\"])  # минимум 20 символов\n",
        "print(f\"После фильтра: {len(chunks_df)} чанков\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRywIn-__WVh",
        "outputId": "d3ce2f10-877a-4976-8687-0a48ce213b4b"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Получено чанков: 8008 (из 1937 страниц)\n",
            "После фильтра: 7987 чанков\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_texts = chunks_df[\"chunk_text\"].tolist()\n",
        "question_texts = questions_df[\"query\"].fillna(\"\").apply(clean_text).tolist()\n",
        "\n",
        "# защита от пустых вопросов\n",
        "question_texts = [q for q in question_texts if q.strip()]\n",
        "if len(question_texts) != len(questions_df):\n",
        "    print(f\"Найдены пустые вопросы. Останется {len(question_texts)} из {len(questions_df)}\")\n",
        "    question_texts = questions_df[\"query\"].fillna(\"placeholder\").apply(clean_text).tolist()\n",
        "\n",
        "print(f\"{len(chunk_texts)} текстов чанков, {len(question_texts)} вопросов\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezXNMENp_a0b",
        "outputId": "4d0fabbc-2f09-446b-ce03-3f71633c0eab"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7987 текстов чанков, 6977 вопросов\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=MAX_FEATURES,\n",
        "    min_df=1,\n",
        "    max_df=MAX_DF,\n",
        "    ngram_range=NGRAM_RANGE,\n",
        "    stop_words=None\n",
        ")\n",
        "\n",
        "chunk_matrix = vectorizer.fit_transform(chunk_texts)          # (num_chunks, vocab_size)\n",
        "question_matrix = vectorizer.transform(question_texts)        # (num_questions, vocab_size)\n",
        "print(f\"Матрица чанков: {chunk_matrix.shape}, Матрица вопросов: {question_matrix.shape}\")\n",
        "\n",
        "#float32 + L2-нормализация для косинусного сходства в IP\n",
        "chunk_dense = chunk_matrix.astype(np.float32).toarray()\n",
        "question_dense = question_matrix.astype(np.float32).toarray()\n",
        "faiss.normalize_L2(chunk_dense)\n",
        "faiss.normalize_L2(question_dense)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0ZuykB8_eWF",
        "outputId": "9864d896-af26-4b2b-d0e8-bfeef4c38a2f"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Матрица чанков: (7987, 20000), Матрица вопросов: (6977, 20000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dim = chunk_dense.shape[1]\n",
        "\n",
        "# создаём HNSW индекс\n",
        "M = 32   # количество связей\n",
        "index = faiss.IndexHNSWFlat(dim, M)\n",
        "\n",
        "# нормализация векторов для косинусного сходства\n",
        "faiss.normalize_L2(chunk_dense)\n",
        "index.add(chunk_dense)\n",
        "\n",
        "print(f\"Индекс HNSW готов, Векторов: {index.ntotal}, размерность: {dim}\")\n",
        "\n",
        "# поиск\n",
        "distances, indices = index.search(question_dense, TOP_K_CHUNKS)\n",
        "print(\"Поиск завершён (HNSW)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXZMHiya_pUs",
        "outputId": "dacb1913-0353-4383-e75f-0c33260a41e6"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Индекс HNSW готов, Векторов: 7987, размерность: 20000\n",
            "Поиск завершён (HNSW)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "for i, q_id in enumerate(questions_df[\"q_id\"]):\n",
        "    # индексы найденных чанков\n",
        "    candidate_web_ids = []\n",
        "    for idx in indices[i]:\n",
        "        if idx == -1:\n",
        "            continue\n",
        "        if 0 <= idx < len(chunks_df):\n",
        "            wid = chunks_df.iloc[idx][\"web_id\"]\n",
        "            if pd.notna(wid) and str(wid).strip() != \"\":\n",
        "                candidate_web_ids.append(str(wid).strip())\n",
        "\n",
        "    # удаляем дубликаты\n",
        "    seen = set()\n",
        "    unique_web_ids = []\n",
        "    for wid in candidate_web_ids:\n",
        "        if wid not in seen:\n",
        "            seen.add(wid)\n",
        "            unique_web_ids.append(wid)\n",
        "\n",
        "    # сокращаем до top_n_webids и дополняем пустыми строками\n",
        "    unique_web_ids = unique_web_ids[:TOP_N_WEBIDS]\n",
        "    while len(unique_web_ids) < TOP_N_WEBIDS:\n",
        "        unique_web_ids.append(\"\")\n",
        "\n",
        "    results.append({\n",
        "        \"q_id\": q_id,\n",
        "        \"web_id_1\": unique_web_ids[0],\n",
        "        \"web_id_2\": unique_web_ids[1],\n",
        "        \"web_id_3\": unique_web_ids[2],\n",
        "        \"web_id_4\": unique_web_ids[3],\n",
        "        \"web_id_5\": unique_web_ids[4],\n",
        "    })\n",
        "\n",
        "submission_df = pd.DataFrame(results)\n",
        "\n",
        "# валидации формата\n",
        "assert list(submission_df.columns) == [\"q_id\",\"web_id_1\",\"web_id_2\",\"web_id_3\",\"web_id_4\",\"web_id_5\"], \"Формат колонок неверный\"\n",
        "assert len(submission_df) == len(questions_df), \"Количество строк не совпадает с количеством вопросов\"\n",
        "\n",
        "submission_df.to_csv(SUBMIT_PATH, index=False)\n",
        "print(f\"Файл сохранён: {SUBMIT_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HReKVTBP_u-3",
        "outputId": "80c18831-7d4f-42b3-bcbc-3de50a9e8f43"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Файл сохранён: submit.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Статистика submit.csv"
      ],
      "metadata": {
        "id": "YfZfKo5OHc_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_nonempty = int((submission_df[[\"web_id_1\",\"web_id_2\",\"web_id_3\",\"web_id_4\",\"web_id_5\"]] != \"\").sum().sum())\n",
        "avg_per_q = total_nonempty / len(submission_df)\n",
        "unique_wids = pd.unique(submission_df[[\"web_id_1\",\"web_id_2\",\"web_id_3\",\"web_id_4\",\"web_id_5\"]].values.ravel())\n",
        "unique_wids = [w for w in unique_wids if isinstance(w, str) and w.strip() != \"\"]\n",
        "print(f\"- Вопросов: {len(submission_df)}\")\n",
        "print(f\"- Уникальных web_id в submit: {len(set(unique_wids))}\")\n",
        "print(f\"- Всего непустых ответов: {total_nonempty}\")\n",
        "print(f\"- Среднее web_id на вопрос: {avg_per_q:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NN9TV5ib_9Nu",
        "outputId": "2ab4be84-6801-498f-e82f-28b2cc21307b"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Вопросов: 6977\n",
            "- Уникальных web_id в submit: 1446\n",
            "- Всего непустых ответов: 34325\n",
            "- Среднее web_id на вопрос: 4.92\n"
          ]
        }
      ]
    }
  ]
}